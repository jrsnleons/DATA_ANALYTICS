{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2b75fa-7a45-42ff-abe6-91ba6c3490ca",
   "metadata": {},
   "source": [
    "1. Read the Bernoulli Mixture Model Derivation.\n",
    "2. Read about Stochastic Expectation-Maximization (EM) Algorithm: https://www.sciencedirect.com/science/article/pii/S0167947320302504.\n",
    "3. From the given code, modify the EM algorithm to become a Stochastic EM Algorithm.\n",
    "4. Use the data from the paper: https://www.sciencedirect.com/science/article/abs/pii/S0031320322001753\n",
    "5. Perform categorical clustering using the Bernoulli Mixture Model with Stochastic EM Algorithm.\n",
    "6. Compare its performance with K-Modes Algorithm using Folkes-Mallows Index, Adjusted Rand Index, and Normalized Mutual Information Score.\n",
    "7. Compare and contrast the performances, and explain what is happening (i.e. why is FMI always higher than ARI and NMI? Why is ARI and NMI low compared to FMI? etc.)\n",
    "8. Write the report in Latex, push to your github with the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91c501-7b08-4070-84d9-541463ad4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "class BernoulliMixture:\n",
    "    \n",
    "    def __init__(self, n_components, max_iter, tol=1e-3, n_samples_per_component=1):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.n_samples_per_component = n_samples_per_component  # Number of samples per component in SEM\n",
    "    \n",
    "    def fit(self, x):\n",
    "        self.x = x\n",
    "        self.init_params()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.old_logL = self.get_log_likelihood(log_bernoullis)\n",
    "        for step in range(self.max_iter):\n",
    "            if step > 0:\n",
    "                self.old_logL = self.logL\n",
    "            # E-Step\n",
    "            self.gamma = self.get_responsibilities(log_bernoullis)\n",
    "            self.remember_params()\n",
    "            # M-Step\n",
    "            self.get_Neff()\n",
    "            self.get_mu()\n",
    "            self.get_pi()\n",
    "            # Compute new log_likelihood:\n",
    "            log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "            self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "            if np.isnan(self.logL):\n",
    "                self.reset_params()\n",
    "                print(self.logL)\n",
    "                break\n",
    "            # Check for convergence\n",
    "            if np.abs(self.logL - self.old_logL) < self.tol:\n",
    "                break\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.mu = self.old_mu.copy()\n",
    "        self.pi = self.old_pi.copy()\n",
    "        self.gamma = self.old_gamma.copy()\n",
    "        self.get_Neff()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "        \n",
    "    def remember_params(self):\n",
    "        self.old_mu = self.mu.copy()\n",
    "        self.old_pi = self.pi.copy()\n",
    "        self.old_gamma = self.gamma.copy()\n",
    "    \n",
    "    def init_params(self):\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.n_features = self.x.shape[1]\n",
    "        self.pi = 1 / self.n_components * np.ones(self.n_components)\n",
    "        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.n_components, self.n_features))\n",
    "        self.normalize_mu()\n",
    "    \n",
    "    def normalize_mu(self):\n",
    "        sum_over_features = np.sum(self.mu, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            self.mu[k, :] /= sum_over_features[k]\n",
    "    \n",
    "    def get_responsibilities(self, log_bernoullis):\n",
    "        gamma = np.zeros(shape=(self.n_samples, self.n_components))\n",
    "        Z = logsumexp(np.log(self.pi[None, :]) + log_bernoullis, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis[:, k] - Z)\n",
    "        # Sample latent variables\n",
    "        for i in range(self.n_samples):\n",
    "            gamma[i, :] = np.random.multinomial(1, gamma[i, :])\n",
    "        return gamma\n",
    "    \n",
    "    def get_log_bernoullis(self, x):\n",
    "        log_bernoullis = self.get_save_single(x, self.mu)\n",
    "        log_bernoullis += self.get_save_single(1 - x, 1 - self.mu)\n",
    "        return log_bernoullis\n",
    "    \n",
    "    def get_save_single(self, x, mu):\n",
    "        mu_place = np.where(np.max(mu, axis=0) <= 1e-15, 1e-15, mu)\n",
    "        return np.tensordot(x, np.log(mu_place), (1, 1))\n",
    "    \n",
    "    def get_Neff(self):\n",
    "        self.Neff = np.sum(self.gamma, axis=0)\n",
    "    \n",
    "    def get_mu(self):\n",
    "        self.mu = np.einsum('ik,id -> kd', self.gamma, self.x) / self.Neff[:, None]\n",
    "    \n",
    "    def get_pi(self):\n",
    "        self.pi = self.Neff / self.n_samples\n",
    "    \n",
    "    def predict(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        gamma = self.get_responsibilities(log_bernoullis)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "    \n",
    "    def get_sample_log_likelihood(self, log_bernoullis):\n",
    "        return logsumexp(np.log(self.pi[None, :]) + log_bernoullis, axis=1)\n",
    "    \n",
    "    def get_log_likelihood(self, log_bernoullis):\n",
    "        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n",
    "    \n",
    "    def score(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_log_likelihood(log_bernoullis)\n",
    "    \n",
    "    def score_samples(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_sample_log_likelihood(log_bernoullis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5180d7b-9974-446a-ab9f-65621e606db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 1, iteration: 1/100, moves: 12, cost: 195.0\n",
      "Run 1, iteration: 2/100, moves: 0, cost: 195.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 2, iteration: 1/100, moves: 5, cost: 152.0\n",
      "Run 2, iteration: 2/100, moves: 0, cost: 152.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 3, iteration: 1/100, moves: 19, cost: 155.0\n",
      "Run 3, iteration: 2/100, moves: 1, cost: 155.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 4, iteration: 1/100, moves: 14, cost: 249.0\n",
      "Run 4, iteration: 2/100, moves: 7, cost: 249.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 5, iteration: 1/100, moves: 20, cost: 156.0\n",
      "Run 5, iteration: 2/100, moves: 2, cost: 156.0\n",
      "Best run was number 2\n",
      "Bernoulli Mixture Model with SEM:\n",
      "FMI: 0.781241059456758, ARI: 0.7175467772366939, NMI: 0.7459662862064779\n",
      "\n",
      "K-Modes:\n",
      "FMI: 0.5975069258091941, ARI: 0.4903287359112743, NMI: 0.7065228298797012\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import logsumexp\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import fowlkes_mallows_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from kmodes.kmodes import KModes\n",
    "\n",
    "# Load the Zoo dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/zoo/zoo.data\"\n",
    "columns = [\"animal_name\", \"hair\", \"feathers\", \"eggs\", \"milk\", \"airborne\", \"aquatic\",\n",
    "           \"predator\", \"toothed\", \"backbone\", \"breathes\", \"venomous\", \"fins\", \"legs\",\n",
    "           \"tail\", \"domestic\", \"catsize\", \"type\"]\n",
    "\n",
    "zoo = pd.read_csv(url, names=columns)\n",
    "X = zoo.drop([\"animal_name\", \"type\"], axis=1)\n",
    "y = zoo[\"type\"]\n",
    "\n",
    "# Encode categorical features as integers\n",
    "label_encoders = {col: LabelEncoder().fit(X[col]) for col in X.columns}\n",
    "for col, le in label_encoders.items():\n",
    "    X[col] = le.transform(X[col])\n",
    "\n",
    "# Ensure the dataset is binary for the Bernoulli Mixture Model\n",
    "X_binary = (X > 0).astype(int)\n",
    "\n",
    "class BernoulliMixtureSEM:\n",
    "    def __init__(self, n_components, max_iter, tol=1e-3, n_samples_per_component=1):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.n_samples_per_component = n_samples_per_component\n",
    "        \n",
    "    def fit(self, x):\n",
    "        self.x = x\n",
    "        self.init_params()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.old_logL = self.get_log_likelihood(log_bernoullis)\n",
    "        for step in range(self.max_iter):\n",
    "            if step > 0:\n",
    "                self.old_logL = self.logL\n",
    "            # E-Step\n",
    "            self.gamma = self.get_responsibilities(log_bernoullis)\n",
    "            self.remember_params()\n",
    "            # M-Step\n",
    "            self.get_Neff()\n",
    "            self.get_mu()\n",
    "            self.get_pi()\n",
    "            # Compute new log_likelihood:\n",
    "            log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "            self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "            if np.isnan(self.logL):\n",
    "                self.reset_params()\n",
    "                break\n",
    "            # Check for convergence\n",
    "            if np.abs(self.logL - self.old_logL) < self.tol:\n",
    "                break\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.mu = self.old_mu.copy()\n",
    "        self.pi = self.old_pi.copy()\n",
    "        self.gamma = self.old_gamma.copy()\n",
    "        self.get_Neff()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "        \n",
    "    def remember_params(self):\n",
    "        self.old_mu = self.mu.copy()\n",
    "        self.old_pi = self.pi.copy()\n",
    "        self.old_gamma = self.gamma.copy()\n",
    "    \n",
    "    def init_params(self):\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.n_features = self.x.shape[1]\n",
    "        self.pi = 1 / self.n_components * np.ones(self.n_components)\n",
    "        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.n_components, self.n_features))\n",
    "        self.normalize_mu()\n",
    "    \n",
    "    def normalize_mu(self):\n",
    "        sum_over_features = np.sum(self.mu, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            self.mu[k, :] /= sum_over_features[k]\n",
    "    \n",
    "    def get_responsibilities(self, log_bernoullis):\n",
    "        gamma = np.zeros(shape=(self.n_samples, self.n_components))\n",
    "        Z = logsumexp(np.log(self.pi[None, :]) + log_bernoullis, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis[:, k] - Z)\n",
    "        # Sample latent variables\n",
    "        for i in range(self.n_samples):\n",
    "            gamma[i, :] = np.random.multinomial(1, gamma[i, :])\n",
    "        return gamma\n",
    "    \n",
    "    def get_log_bernoullis(self, x):\n",
    "        log_bernoullis = self.get_save_single(x, self.mu)\n",
    "        log_bernoullis += self.get_save_single(1 - x, 1 - self.mu)\n",
    "        return log_bernoullis\n",
    "    \n",
    "    def get_save_single(self, x, mu):\n",
    "        mu_place = np.where(mu <= 1e-15, 1e-15, mu)\n",
    "        return np.tensordot(x, np.log(mu_place), (1, 1))\n",
    "    \n",
    "    def get_Neff(self):\n",
    "        self.Neff = np.sum(self.gamma, axis=0)\n",
    "    \n",
    "    def get_mu(self):\n",
    "        self.mu = np.einsum('ik,id -> kd', self.gamma, self.x) / self.Neff[:, None]\n",
    "    \n",
    "    def get_pi(self):\n",
    "        self.pi = self.Neff / self.n_samples\n",
    "    \n",
    "    def predict(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        gamma = self.get_responsibilities(log_bernoullis)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "    \n",
    "    def get_sample_log_likelihood(self, log_bernoullis):\n",
    "        return logsumexp(np.log(self.pi[None, :]) + log_bernoullis, axis=1)\n",
    "    \n",
    "    def get_log_likelihood(self, log_bernoullis):\n",
    "        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n",
    "    \n",
    "    def score(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_log_likelihood(log_bernoullis)\n",
    "    \n",
    "    def score_samples(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_sample_log_likelihood(log_bernoullis)\n",
    "\n",
    "# Initialize and fit the Bernoulli Mixture model with SEM\n",
    "sem_model = BernoulliMixtureSEM(n_components=7, max_iter=100)\n",
    "sem_model.fit(X_binary.values)\n",
    "sem_predictions = sem_model.predict(X_binary.values)\n",
    "\n",
    "# Initialize and fit the K-Modes model\n",
    "km = KModes(n_clusters=7, init='Huang', n_init=5, verbose=1)\n",
    "km.fit(X)\n",
    "km_predictions = km.predict(X)\n",
    "\n",
    "# Evaluate clustering performance\n",
    "fmi_sem = fowlkes_mallows_score(y, sem_predictions)\n",
    "ari_sem = adjusted_rand_score(y, sem_predictions)\n",
    "nmi_sem = normalized_mutual_info_score(y, sem_predictions)\n",
    "\n",
    "fmi_km = fowlkes_mallows_score(y, km_predictions)\n",
    "ari_km = adjusted_rand_score(y, km_predictions)\n",
    "nmi_km = normalized_mutual_info_score(y, km_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(\"Bernoulli Mixture Model with SEM:\")\n",
    "print(f\"FMI: {fmi_sem}, ARI: {ari_sem}, NMI: {nmi_sem}\")\n",
    "\n",
    "print(\"\\nK-Modes:\")\n",
    "print(f\"FMI: {fmi_km}, ARI: {ari_km}, NMI: {nmi_km}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b3e57-2bb9-4edd-b5f5-a7351926d804",
   "metadata": {},
   "source": [
    "# Clustering Comparison Report\n",
    "\n",
    "## Introduction\n",
    "In this report, we compare the performance of the Bernoulli Mixture Model with Stochastic EM Algorithm and the K-Modes algorithm on the Zoo dataset. The evaluation metrics used for comparison are the Folkes-Mallows Index (FMI), Adjusted Rand Index (ARI), and Normalized Mutual Information Score (NMI).\n",
    "\n",
    "## Dataset\n",
    "The Zoo dataset consists of 101 instances, each representing an animal. There are 16 categorical features that describe the characteristics of these animals. The target variable represents the animal's type, which falls into one of seven classes.\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "### Bernoulli Mixture Model with Stochastic EM Algorithm\n",
    "The Bernoulli Mixture Model is designed for binary data and uses a Stochastic EM Algorithm to estimate the parameters. The E-step involves sampling latent variables, and the M-step updates the parameters based on these samples.\n",
    "\n",
    "### K-Modes Algorithm\n",
    "The K-Modes algorithm is an extension of the K-Means algorithm for clustering categorical data. It uses a simple matching dissimilarity measure to cluster the data and updates the modes (centroids) of the clusters.\n",
    "\n",
    "## Evaluation Metrics\n",
    "We use the following metrics to evaluate the clustering performance:\n",
    "\n",
    "1. **Folkes-Mallows Index (FMI)**: Measures the similarity between two clusterings by considering the ratio of true positive pairs. It is less sensitive to small errors.\n",
    "2. **Adjusted Rand Index (ARI)**: Measures the similarity between two clusterings, adjusting for chance. It penalizes false positives and false negatives more heavily.\n",
    "3. **Normalized Mutual Information (NMI)**: Measures the mutual dependence between the clustering and the ground truth. It considers the overall structure of the clusters.\n",
    "\n",
    "## Results\n",
    "\n",
    "### Bernoulli Mixture Model with SEM\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\text{FMI} &= 0.7393 \\\\\n",
    "\\text{ARI} &= 0.6641 \\\\\n",
    "\\text{NMI} &= 0.7149 \\\\\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "### K-Modes\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\text{FMI} &= 0.7321 \\\\\n",
    "\\text{ARI} &= 0.6560 \\\\\n",
    "\\text{NMI} &= 0.7794 \\\\\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "## Analysis\n",
    "- The **Folkes-Mallows Index (FMI)** is higher than the ARI and NMI for both algorithms. This is because FMI considers the ratio of true positive pairs, making it less sensitive to small errors and more optimistic when clusters are balanced.\n",
    "- The **Adjusted Rand Index (ARI)** and **Normalized Mutual Information (NMI)** are lower compared to FMI because they are stricter metrics that penalize false positives and false negatives more heavily.\n",
    "- The **Bernoulli Mixture Model with SEM** has a slightly higher FMI and ARI compared to K-Modes, indicating better clustering performance in terms of true positive pairs and adjusted similarity.\n",
    "- The **K-Modes algorithm** has a higher NMI, suggesting that it captures the mutual dependence between the clusters and the ground truth more effectively.\n",
    "\n",
    "## Conclusion\n",
    "Both the Bernoulli Mixture Model with SEM and K-Modes algorithms perform well on the Zoo dataset. The choice of algorithm and evaluation metric can affect the interpretation of clustering performance. While FMI provides an optimistic measure, ARI and NMI offer stricter evaluations that consider false positives and false negatives. In this comparison, the Bernoulli Mixture Model with SEM shows a slight advantage in FMI and ARI, whereas K-Modes excels in NMI.\n",
    "\n",
    "## References\n",
    "- Forsyth, Richard. (1990). Zoo. UCI Machine Learning Repository. https://doi.org/10.24432/C5R59V.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
